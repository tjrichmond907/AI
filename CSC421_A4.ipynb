{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Fall 2021 Assignment 4 \n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the topics covered in **Chapter 14 - Probabilistic Reasoning over Time**, **Chapter 20 - Learning probabilistic models**, and **Chapter 19 Learning from Examples** from the book *Artificial Intelligence: A Modern Approach.*  You are welcome and actually it can be educational to look at the code at the aima-code repository as well as other code resources you can find on the web. However, make sure you understand any code that you incoporate. \n",
    "\n",
    "The assignment structure is as follows - each item is worth 1 point: \n",
    "\n",
    "1. Bayesian Network  (Basic) - express network and print CPT  \n",
    "2. Bayesian Network  (Expected) - markdown and direct inference   \n",
    "3. Bayesian Network  (Basic) -  approximate inference (rejection sampling and likelihood weighting) \n",
    "4. Bayesian Netowrk  (Advanced) - naive bayes of movie reviews as bayesian network \n",
    "5. Hidden Markov Models (Basic) - Use HMM to generate plausible DNA sequences and visualize \n",
    "6. Hidden Markov Models (Expected) - Learn HMM from samples for DNA sequences \n",
    "7. Hidden Markov Model (Expected) - Compare classification accuracy of ignoring transition matrix \n",
    "8. Hidden Markov Models (Advanced) - make up HMM scenario for activity detection using 2D coordinates and GMMs  \n",
    "9. Classification (Basic) - Replicate movie review classification using bernoulli Naive Bayes in sklearn \n",
    "10. Classification(Expected) - Explore a standard classification problem with continuous attributes in sklearn \n",
    "\n",
    "The grading will be done in 0.5 increments. 1 point for correct answer, 0.5 points for partial or incorrect \n",
    "but reasonable answer and 0.0 for no answer or completely wrong answer. \n",
    "\n",
    "**Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.** \n",
    "\n",
    "**Gould, Stephen Jay** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 (Basic)  - 1 point\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"dispnea.png\">\n",
    "\n",
    "Using the convetions for DBNs used in probability.ipynb (from the AIMA authors) encode the diapnea network shown above. Once you have constructed the Bayesian network display the cpt for the Lung Cancer Node (using the API provided not just showing the numbers).\n",
    "\n",
    "The cell below contains the code that defined BayesNodes and BayesNetworks and the following cell \n",
    "shows an example of defining the Burglary network and performing a query using direct enumeration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (1.21.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "\n",
    "def extend(s, var, val):\n",
    "    \"\"\"Copy dict s and extend it by setting var to val; return copy.\"\"\"\n",
    "    return {**s, var: val}\n",
    "\n",
    "def event_values(event, variables):                                                                      \n",
    "    \"\"\"Return a tuple of the values of variables in event.                                               \n",
    "    >>> event_values ({'A': 10, 'B': 9, 'C': 8}, ['C', 'A'])                                             \n",
    "    (8, 10)                                                                                              \n",
    "    >>> event_values ((1, 2), ['C', 'A'])                                                                \n",
    "    (1, 2)                                                                                               \n",
    "    \"\"\"                                                                                                  \n",
    "    if isinstance(event, tuple) and len(event) == len(variables):                                        \n",
    "        return event                                                                                     \n",
    "    else:                                                                                                \n",
    "        return tuple([event[var] for var in variables])                                                  \n",
    "                      \n",
    "def probability(p):                                                                                      \n",
    "    \"\"\"Return true with probability p.\"\"\"                                                                \n",
    "    return p > random.uniform(0.0, 1.0)  \n",
    "        \n",
    "class ProbDist:\n",
    "    \"\"\"A discrete probability distribution. You name the random variable\n",
    "    in the constructor, then assign and query probability of values.\n",
    "    >>> P = ProbDist('Flip'); P['H'], P['T'] = 0.25, 0.75; P['H']\n",
    "    0.25\n",
    "    >>> P = ProbDist('X', {'lo': 125, 'med': 375, 'hi': 500})\n",
    "    >>> P['lo'], P['med'], P['hi']\n",
    "    (0.125, 0.375, 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, var_name='?', freq=None):\n",
    "        \"\"\"If freq is given, it is a dictionary of values - frequency pairs,\n",
    "        then ProbDist is normalized.\"\"\"\n",
    "        self.prob = {}\n",
    "        self.var_name = var_name\n",
    "        self.values = []\n",
    "        if freq:\n",
    "            for (v, p) in freq.items():\n",
    "                self[v] = p\n",
    "            self.normalize()\n",
    "\n",
    "    def __getitem__(self, val):\n",
    "        \"\"\"Given a value, return P(value).\"\"\"\n",
    "        try:\n",
    "            return self.prob[val]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "\n",
    "    def __setitem__(self, val, p):\n",
    "        \"\"\"Set P(val) = p.\"\"\"\n",
    "        if val not in self.values:\n",
    "            self.values.append(val)\n",
    "        self.prob[val] = p\n",
    "\n",
    "    def normalize(self):\n",
    "        \"\"\"Make sure the probabilities of all values sum to 1.\n",
    "        Returns the normalized distribution.\n",
    "        Raises a ZeroDivisionError if the sum of the values is 0.\"\"\"\n",
    "        total = sum(self.prob.values())\n",
    "        if not np.isclose(total, 1.0):\n",
    "            for val in self.prob:\n",
    "                self.prob[val] /= total\n",
    "        return self\n",
    "\n",
    "    def show_approx(self, numfmt='{:.3g}'):\n",
    "        \"\"\"Show the probabilities rounded and sorted by key, for the\n",
    "        sake of portable doctests.\"\"\"\n",
    "        return ', '.join([('{}: ' + numfmt).format(v, p) for (v, p) in sorted(self.prob.items())])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"P({})\".format(self.var_name)\n",
    "\n",
    "\n",
    "class BayesNode:\n",
    "    \"\"\"A conditional probability distribution for a boolean variable,\n",
    "    P(X | parents). Part of a BayesNet.\"\"\"\n",
    "\n",
    "    def __init__(self, X, parents, cpt):\n",
    "        \"\"\"X is a variable name, and parents a sequence of variable\n",
    "        names or a space-separated string. cpt, the conditional\n",
    "        probability table, takes one of these forms:\n",
    "\n",
    "        * A number, the unconditional probability P(X=true). You can\n",
    "          use this form when there are no parents.\n",
    "\n",
    "        * A dict {v: p, ...}, the conditional probability distribution\n",
    "          P(X=true | parent=v) = p. When there's just one parent.\n",
    "\n",
    "        * A dict {(v1, v2, ...): p, ...}, the distribution P(X=true |\n",
    "          parent1=v1, parent2=v2, ...) = p. Each key must have as many\n",
    "          values as there are parents. You can use this form always;\n",
    "          the first two are just conveniences.\n",
    "\n",
    "        In all cases the probability of X being false is left implicit,\n",
    "        since it follows from P(X=true).\n",
    "\n",
    "        >>> X = BayesNode('X', '', 0.2)\n",
    "        >>> Y = BayesNode('Y', 'P', {T: 0.2, F: 0.7})\n",
    "        >>> Z = BayesNode('Z', 'P Q',\n",
    "        ...    {(T, T): 0.2, (T, F): 0.3, (F, T): 0.5, (F, F): 0.7})\n",
    "        \"\"\"\n",
    "        if isinstance(parents, str):\n",
    "            parents = parents.split()\n",
    "\n",
    "        # We store the table always in the third form above.\n",
    "        if isinstance(cpt, (float, int)):  # no parents, 0-tuple\n",
    "            cpt = {(): cpt}\n",
    "        elif isinstance(cpt, dict):\n",
    "            # one parent, 1-tuple\n",
    "            if cpt and isinstance(list(cpt.keys())[0], bool):\n",
    "                cpt = {(v,): p for v, p in cpt.items()}\n",
    "\n",
    "        assert isinstance(cpt, dict)\n",
    "        for vs, p in cpt.items():\n",
    "            assert isinstance(vs, tuple) and len(vs) == len(parents)\n",
    "            assert all(isinstance(v, bool) for v in vs)\n",
    "            assert 0 <= p <= 1\n",
    "\n",
    "        self.variable = X\n",
    "        self.parents = parents\n",
    "        self.cpt = cpt\n",
    "        self.children = []\n",
    "\n",
    "    def p(self, value, event):\n",
    "        \"\"\"Return the conditional probability\n",
    "        P(X=value | parents=parent_values), where parent_values\n",
    "        are the values of parents in event. (event must assign each\n",
    "        parent a value.)\n",
    "        >>> bn = BayesNode('X', 'Burglary', {T: 0.2, F: 0.625})\n",
    "        >>> bn.p(False, {'Burglary': False, 'Earthquake': True})\n",
    "        0.375\"\"\"\n",
    "        assert isinstance(value, bool)\n",
    "        ptrue = self.cpt[event_values(event, self.parents)]\n",
    "        return ptrue if value else 1 - ptrue\n",
    "\n",
    "    def sample(self, event):\n",
    "        \"\"\"Sample from the distribution for this variable conditioned\n",
    "        on event's values for parent_variables. That is, return True/False\n",
    "        at random according with the conditional probability given the\n",
    "        parents.\"\"\"\n",
    "        return probability(self.p(True, event))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr((self.variable, ' '.join(self.parents)))\n",
    "    \n",
    "    \n",
    "class BayesNet:\n",
    "    \"\"\"Bayesian network containing only boolean-variable nodes.\"\"\"\n",
    "\n",
    "    def __init__(self, node_specs=None):\n",
    "        \"\"\"Nodes must be ordered with parents before children.\"\"\"\n",
    "        self.nodes = []\n",
    "        self.variables = []\n",
    "        node_specs = node_specs or []\n",
    "        for node_spec in node_specs:\n",
    "            self.add(node_spec)\n",
    "\n",
    "    def add(self, node_spec):\n",
    "        \"\"\"Add a node to the net. Its parents must already be in the\n",
    "        net, and its variable must not.\"\"\"\n",
    "        node = BayesNode(*node_spec)\n",
    "        assert node.variable not in self.variables\n",
    "        assert all((parent in self.variables) for parent in node.parents)\n",
    "        self.nodes.append(node)\n",
    "        self.variables.append(node.variable)\n",
    "        for parent in node.parents:\n",
    "            self.variable_node(parent).children.append(node)\n",
    "\n",
    "    def variable_node(self, var):\n",
    "        \"\"\"Return the node for the variable named var.\n",
    "        >>> burglary.variable_node('Burglary').variable\n",
    "        'Burglary'\"\"\"\n",
    "        for n in self.nodes:\n",
    "            if n.variable == var:\n",
    "                return n\n",
    "        raise Exception(\"No such variable: {}\".format(var))\n",
    "\n",
    "    def variable_values(self, var):\n",
    "        \"\"\"Return the domain of var.\"\"\"\n",
    "        return [True, False]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'BayesNet({0!r})'.format(self.nodes)\n",
    "    \n",
    "    \n",
    "def enumerate_all(variables, e, bn):\n",
    "    \"\"\"Return the sum of those entries in P(variables | e{others})\n",
    "    consistent with e, where P is the joint distribution represented\n",
    "    by bn, and e{others} means e restricted to bn's other variables\n",
    "    (the ones other than variables). Parents must precede children in variables.\"\"\"\n",
    "    if not variables:\n",
    "        return 1.0\n",
    "    Y, rest = variables[0], variables[1:]\n",
    "    Ynode = bn.variable_node(Y)\n",
    "    if Y in e:\n",
    "        return Ynode.p(e[Y], e) * enumerate_all(rest, e, bn)\n",
    "    else:\n",
    "        return sum(Ynode.p(y, e) * enumerate_all(rest, extend(e, Y, y), bn)\n",
    "                   for y in bn.variable_values(Y))\n",
    "\n",
    "def enumeration_ask(X, e, bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.9]\n",
    "    Return the conditional probability distribution of variable X\n",
    "    given evidence e, from BayesNet bn.\n",
    "    >>> enumeration_ask('Burglary', dict(JohnCalls=T, MaryCalls=T), burglary\n",
    "    ...  ).show_approx()\n",
    "    'False: 0.716, True: 0.284'\"\"\"\n",
    "    assert X not in e, \"Query variable must be distinct from evidence\"\n",
    "    Q = ProbDist(X)\n",
    "    for xi in bn.variable_values(X):\n",
    "        Q[xi] = enumerate_all(bn.variables, extend(e, X, xi), bn)\n",
    "    return Q.normalize()\n",
    "\n",
    "def consistent_with(event, evidence):\n",
    "    \"\"\"Is event consistent with the given evidence?\"\"\"\n",
    "    return all(evidence.get(k, v) == v for k, v in event.items())\n",
    "\n",
    "def prior_sample(bn):\n",
    "    \"\"\"\n",
    "    [Figure 14.13]\n",
    "    Randomly sample from bn's full joint distribution.\n",
    "    The result is a {variable: value} dict.\n",
    "    \"\"\"\n",
    "    event = {}\n",
    "    for node in bn.nodes:\n",
    "        event[node.variable] = node.sample(event)\n",
    "    return event\n",
    "\n",
    "def rejection_sampling(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.14]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn, using N samples.\n",
    "    Raises a ZeroDivisionError if all the N samples are rejected,\n",
    "    i.e., inconsistent with e.\n",
    "    >>> random.seed(47)\n",
    "    >>> rejection_sampling('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.7, True: 0.3'\n",
    "    \"\"\"\n",
    "    counts = {x: 0 for x in bn.variable_values(X)}  # bold N in [Figure 14.14]\n",
    "    for j in range(N):\n",
    "        sample = prior_sample(bn)  # boldface x in [Figure 14.14]\n",
    "        if consistent_with(sample, e):\n",
    "            counts[sample[X]] += 1\n",
    "    return ProbDist(X, counts)\n",
    "\n",
    "def weighted_sample(bn, e):\n",
    "    \"\"\"\n",
    "    Sample an event from bn that's consistent with the evidence e;\n",
    "    return the event and its weight, the likelihood that the event\n",
    "    accords to the evidence.\n",
    "    \"\"\"\n",
    "    w = 1\n",
    "    event = dict(e)  # boldface x in [Figure 14.15]\n",
    "    for node in bn.nodes:\n",
    "        Xi = node.variable\n",
    "        if Xi in e:\n",
    "            w *= node.p(e[Xi], event)\n",
    "        else:\n",
    "            event[Xi] = node.sample(event)\n",
    "    return event, w\n",
    "\n",
    "def likelihood_weighting(X, e, bn, N=10000):\n",
    "    \"\"\"\n",
    "    [Figure 14.15]\n",
    "    Estimate the probability distribution of variable X given\n",
    "    evidence e in BayesNet bn.\n",
    "    >>> random.seed(1017)\n",
    "    >>> likelihood_weighting('Burglary', dict(JohnCalls=T, MaryCalls=T),\n",
    "    ...   burglary, 10000).show_approx()\n",
    "    'False: 0.702, True: 0.298'\n",
    "    \"\"\"\n",
    "    W = {x: 0 for x in bn.variable_values(X)}\n",
    "    for j in range(N):\n",
    "        sample, weight = weighted_sample(bn, e)  # boldface x, w in [Figure 14.15]\n",
    "        W[sample[X]] += weight\n",
    "    return ProbDist(X, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}\n",
      "0.2841718353643929 0.7158281646356071\n",
      "0.35294117647058826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.744, True: 0.256'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   burglary = BayesNet([\n",
    "        ('Burglary', '', 0.001),\n",
    "        ('Earthquake', '', 0.002),\n",
    "        ('Alarm', 'Burglary Earthquake',\n",
    "         {(True, True): 0.95, (True, False): 0.94, (False, True): 0.29, (False, False): 0.001}),\n",
    "        ('JohnCalls', 'Alarm', {True: 0.90, False: 0.05}),\n",
    "        ('MaryCalls', 'Alarm', {True: 0.70, False: 0.01})\n",
    "    ])\n",
    "    \n",
    "print(burglary.variable_node('Alarm').cpt)\n",
    "ans_dist = enumeration_ask('Burglary', {'JohnCalls': True, 'MaryCalls': True}, burglary)\n",
    "print(ans_dist[True],ans_dist[False])\n",
    "\n",
    "\n",
    "p = rejection_sampling('Burglary', dict(JohnCalls=True, MaryCalls=True), burglary, 10000)\n",
    "print(p[True])\n",
    "\n",
    "likelihood_weighting('Burglary', dict(JohnCalls=True, MaryCalls=True),burglary, 10000).show_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(True,): 0.1, (False,): 0.01}\n",
      "Conditional Probability Table for Lung Cancer\n",
      "+----------+------------+----------------+\n",
      "|----------|lung cancer | not lung cancer|\n",
      "|smoker    |    0.1     |       0.9      |\n",
      "|not smoker|    0.01    |       0.99     |\n",
      "+----------+------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "dispnea = BayesNet([\n",
    "        ('VisitAsia', '', 0.001),\n",
    "        ('Smoker', '', 0.5),\n",
    "        ('Tuberculosis', 'VisitAsia', {True: 0.05, False: 0.01}), \n",
    "        ('LungCancer', 'Smoker', {True: 0.1, False: 0.01}),\n",
    "        ('Bronchitis', 'Smoker', {True: 0.6, False: 0.3}),\n",
    "        ('EitherTorL', 'Tuberculosis LungCancer', {(True, True): 1, (True, False): 1, (False, True): 1, (False, False): 0}),\n",
    "        ('PosXray', 'EitherTorL', {True: 0.98, False: 0.05}),\n",
    "        ('Dispnea', 'EitherTorL Bronchitis', {(True, True): 0.9, (True, False): 0.7, (False, True): 0.8, (False, False): 0.1})\n",
    "    ])\n",
    "\n",
    "print(dispnea.variable_node('LungCancer').cpt)\n",
    "p = dispnea.variable_node('LungCancer').cpt\n",
    "print('Conditional Probability Table for Lung Cancer')\n",
    "print('+----------+------------+----------------+')\n",
    "print('|----------|lung cancer | not lung cancer|')\n",
    "print('|smoker    |    {}     |       {}      |'.format(p[True,], 1 - p[True,]))\n",
    "print('|not smoker|    {}    |       {}     |'.format(p[False,], 1 - p[False,]))\n",
    "print('+----------+------------+----------------+')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 (Expected) 1 point \n",
    "\n",
    "Answer using exact inference with enumeration the following query: given that a patient has been in Asia and has a positive xray, what is the likelihood of having dispnea?\n",
    "\n",
    "Write down using markdown the expression that corresponds to this query and the corresponding numbers from the CPT. There will be multiple sums and subscripts. Calculate the result using a calculator.\n",
    "\n",
    "Write code for the same query using enumeration_ask and confirm that the result is the same for the same query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#using variable elimination:\n",
    "\n",
    "$P(d|a,x) = \\Sigma_{A,S,T,L,B,E,X,D} P(d|e,b) * P(e|t,l) * P(b|s) * P(l|s) *P(t|a) * P(s) * P(a)$\n",
    "\n",
    "$\\Sigma_{A} P(t|a) * P(a) = f_{1}(t)$\n",
    "\n",
    "$         = \\Sigma_{S,T,L,B,E,X,D} P(d|e,b) * P(e|t,l) * P(b|s) * P(l|s) * P(s) * f_{1}(t)$\n",
    "\n",
    "$\\Sigma_{X} P(X) = 1$\n",
    "\n",
    "$\\Sigma_{S} P(b|s) * P(l|s) * P(s) = f_{2}(b,l)$\n",
    "\n",
    "$          = \\Sigma_{T,L,B,E} P(d|e,b) * P(e|t,l) *f_{1}(t) * f_{2}(b,l)$\n",
    "\n",
    "$\\Sigma_{T} P(e|l,t) * f_{1}(t) = f_{3}(a,l)$\n",
    "\n",
    "$          = \\Sigma_{L,B,E} P(d|b,e) * f_{2}(b,l) * f_{3}(a,l)$\n",
    "\n",
    "$\\Sigma_{L} f_{2}(b,l) * f_{3}(a,l) = f_{4}(a,b)$\n",
    "\n",
    "$          = \\Sigma_{B,E} P(d|e,b) * f_{4}(a,b)$\n",
    "\n",
    "$\\Sigma_{E}\\Sigma_{B} P(d|e,b) * f_{4}(a,b) = f_{5}(a)$\n",
    "\n",
    "$          = \\Sigma_{E} f_{5}(a)$\n",
    "\n",
    "$ = aprroximately   0.68$\n",
    "\n",
    "Mutlitplying (across rows) and summing (final column) the values using given VisitAsia and PosXray as true to find the probability of having dyspena. Lowercase letters indicate false random variables and uppercase letters represent true random variables. Probablity table headings are in this order:  \n",
    "P(d|e,b) P(x|e) P(b|s) P(e|l,t) P(l|s) P(T|A) P(A) P(S) (probability)\n",
    "\n",
    "Table for variable multiplication:  \n",
    "  \n",
    "\n",
    "1. STBLE 0.9  0.98  0.6   1   0.1  0.05  0.01  0.5  0.00001323 \n",
    "\n",
    "2. STBLe 0.8  0.05  0.6   0   0.1  0.05  0.01  0.5  0   \n",
    "\n",
    "3. STBlE 0.9  0.98  0.6   1   0.9  0.05  0.01  0.5  0.00011907  \n",
    "\n",
    "4. STBle 0.8  0.05  0.6   0   0.9  0.05  0.01  0.5  0   \n",
    "\n",
    "5. STbLE 0.7  0.98  0.4   1   0.1  0.05  0.01  0.5  0.00000686  \n",
    "\n",
    "6. STbLe 0.1  0.05  0.4   0   0.1  0.05  0.01  0.5  0   \n",
    "\n",
    "7. STblE 0.7  0.98  0.4   1   0.9  0.05  0.01  0.5  0.00006174   \n",
    "\n",
    "8. STble 0.1  0.05  0.4   0   0.9  0.05  0.01  0.5  0  \n",
    "\n",
    "9. StBLE 0.9  0.98  0.6   1   0.1  0.95  0.01  0.5  0.00025137  \n",
    "\n",
    "10. StBLe 0.8  0.05  0.6   0   0.1  0.95  0.01  0.5  0   \n",
    "\n",
    "11. StBlE 0.9  0.98  0.6   0   0.9  0.95  0.01  0.5  0    \n",
    "\n",
    "12. StBle 0.8  0.05  0.6   1   0.9  0.95  0.01  0.5  0.0001026   \n",
    "\n",
    "13. StbLE 0.7  0.98  0.4   1   0.1  0.95  0.01  0.5  0.00013034  \n",
    "\n",
    "14. StbLe 0.1  0.05  0.4   0   0.1  0.95  0.01  0.5  0   \n",
    "\n",
    "15. StblE 0.7  0.98  0.4   0   0.99  0.95  0.01  0.5  0   \n",
    "\n",
    "16. Stble 0.1  0.05  0.4   1   0.99  0.95  0.01  0.5  0.000009405  \n",
    "\n",
    "17. sTBLE 0.9  0.98  0.3   1   0.01  0.05  0.01  0.5  0.000000661  \n",
    "\n",
    "18. sTBLe 0.8  0.05  0.3   0   0.01  0.05  0.01  0.5  0   \n",
    "\n",
    "19. sTBlE 0.9  0.98  0.3   1   0.99  0.05  0.01  0.5  0.000065488   \n",
    "\n",
    "20. sTBle 0.8  0.05  0.3   0   0.99  0.05  0.01  0.5  0  \n",
    "\n",
    "21. sTbLE 0.7  0.98  0.7   1   0.01  0.05  0.01  0.5  0.0000012  \n",
    "\n",
    "22. sTbLe 0.1  0.05  0.7   0   0.01  0.05  0.01  0.5  0   \n",
    "\n",
    "23. sTblE 0.7  0.98  0.7   1   0.99  0.05  0.01  0.5  0.00011849  \n",
    "\n",
    "24. sTble 0.1  0.05  0.7   0   0.99  0.05  0.01  0.5  0   \n",
    "\n",
    "25. stBLE 0.9  0.98  0.3   1   0.01  0.95  0.01  0.5  0.000012568   \n",
    "\n",
    "26. stBLe 0.8  0.05  0.3   0   0.01  0.95  0.01  0.5  0   \n",
    "\n",
    "27. stBlE 0.9  0.98  0.3   0   0.99  0.95  0.01  0.5  0   \n",
    "\n",
    "28. stBle 0.8  0.05  0.3   1   0.99  0.95  0.01  0.5  0.00005643   \n",
    "\n",
    "29. stbLE 0.7  0.98  0.7   1   0.01  0.95  0.01  0.5  0.000022809  \n",
    "\n",
    "30. stbLe 0.1  0.05  0.7   0   0.01  0.95  0.01  0.5  0   \n",
    "\n",
    "31. stblE 0.7  0.98  0.7   0   0.99  0.95  0.01  0.5  0   \n",
    " \n",
    "32. stble 0.1  0.05  0.7   1   0.99  0.95  0.01  0.5  0.000016458  \n",
    "  \n",
    "The summation of the probabilities is: 0.000870229 which is not yet normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6811011940658546 0.3188988059341455\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "\n",
    "ans = enumeration_ask('Dispnea', {'VisitAsia': True, 'PosXray': True}, dispnea)\n",
    "print(ans[True], ans[False])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 3 (Basic) - 1 point\n",
    "\n",
    "Answer using approximate inference the same query using both rejection sampling and likelihood weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rejection sampling 10 times\n",
      "1: The probability distribution of variable Dispnea given the evidence is: 0\n",
      "2: The probability distribution of variable Dispnea given the evidence is: 0.6666666666666666\n",
      "3: All N samples were rejected in rejection sampling.\n",
      "4: The probability distribution of variable Dispnea given the evidence is: 0.5\n",
      "5: The probability distribution of variable Dispnea given the evidence is: 0.5\n",
      "6: The probability distribution of variable Dispnea given the evidence is: 0.5\n",
      "7: All N samples were rejected in rejection sampling.\n",
      "8: The probability distribution of variable Dispnea given the evidence is: 0.0\n",
      "9: The probability distribution of variable Dispnea given the evidence is: 0.5\n",
      "10: The probability distribution of variable Dispnea given the evidence is: 0.5\n",
      "Testing using liklihood weighting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'False: 0.323, True: 0.677'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "#running rejection sampling a few times to show that there is variablility \n",
    "#in the results depending on the samples taken\n",
    "print('Testing rejection sampling 10 times')\n",
    "for i in range(0,10):\n",
    "    try: \n",
    "        p = rejection_sampling('Dispnea', dict(VisitAsia=True, PosXray=True), dispnea, 10000)\n",
    "        print('{}: The probability distribution of variable Dispnea given the evidence is: {}'.format(i+1,p[True]))\n",
    "        \n",
    "    except ZeroDivisionError:\n",
    "        print('{}: All N samples were rejected in rejection sampling.'.format(i+1))\n",
    "        continue\n",
    "        \n",
    "print('Testing using liklihood weighting')\n",
    "likelihood_weighting('Dispnea', dict(VisitAsia=True, PosXray=True), dispnea, 10000).show_approx()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 4 (ADVANCED) - 1 point \n",
    "\n",
    "A Naive Bayes classifier can be considered as a Bayesian Network. The classification problem can then be expressed as setting all the variables corresponding to the features as evidence and querying the probability for the class. Express the Bernoulli Naive Bayes classifier you implemented in the previous assignment as a Bayesian Network using the probability.ipynb conventions used in this notebook. Now that you have a DBN express and solve the classification problem as a query and go over all the previous steps for this particular problem. More specifically do exact inference by enumeration, approximate inference by rejection sampling to answer the query and show the results. Use 4 specific examples (2 positive and 2 negative) from the training dataset to show how the prediction using the Bayesian network works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 (Basic) -1 point\n",
    "\n",
    "\n",
    "The next three question explore hidden markov models (HMMs) and use the hmmlearn Python library. You can use the code for the weather example in the probabilistic reasoning over time notebook we covered in class as a template for writing your code. \n",
    "\n",
    "The problem used in inspired by the use of HMMs in bioinformatics. \n",
    "There are several simplifications made to make it reasonable as part of an assignment. DNA sequences can be considered strings over an alphabet of 4 symbols/nucleobases **A,C,T,G (adenine, cytosine, thymine, guanine**. Parts of a DNA sequence are dense with C and G and other parts are sparse with C and G and it is of interest to biologists to identify these regions. \n",
    "\n",
    "We will model the CG-dense **(CGD)** and **CG-sparse** (CGS) as hidden states and the nucleobases are the observations. Through experimental data we have the following information: \n",
    "\n",
    "1. The transition probability from CGD to CGS is 0.37 and the probability of staying in CGD is 0.63. The transition probability from CGS to CGD is similarly 0.37 with 0.63 being the probability of staying in CGS. \n",
    "\n",
    "2. The observation probabilities of CGD regions are: A: 0.15, C:0.35, G: 0.35, and T:0.15. The observation probabilities of CGS regions are: A: 0.40, C: 0.10, G: 0.10, T: 0.40 \n",
    "\n",
    "3. You can assume that the initial state probabilities are the same (0.5) \n",
    "\n",
    "4. For visualization of the DNA sequences use the following color mapping: A: red, C: green, T: blue, G: yellow, and for CGD: black \n",
    "and CGS: white \n",
    "\n",
    "\n",
    "Define this HMM model using the **hmmlearn** conventions. Then use the created model to generate a sequence of 1000 samples (i.e both hidden states and corresponding observations). Use the colors above \n",
    "to visualize the sequence of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "  Using cached hmmlearn-0.2.6-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (369 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.9/site-packages (from hmmlearn) (1.21.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting setuptools-scm>=4\n",
      "  Using cached setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.28.3-py3-none-any.whl (884 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from matplotlib) (1.21.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib) (49.6.0.post20210108)\n",
      "Collecting tomli>=1.0.0\n",
      "  Using cached tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: tomli, setuptools-scm, pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.3 kiwisolver-1.3.2 matplotlib-3.5.0 pillow-8.4.0 setuptools-scm-6.3.2 tomli-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "import numpy as np \n",
    "from hmmlearn import hmm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_DNA_samples(samples, state2color, title): \n",
    "    colors = [state2color[x] for x in samples]\n",
    "    x = np.arange(0, len(colors))\n",
    "    y = np.ones(len(colors))\n",
    "    plt.figure(figsize=(10,1))\n",
    "    plt.bar(x, y, color=colors, width=1)\n",
    "    plt.title(title)\n",
    "\n",
    "#######hmmlearn---------------------------------\n",
    "\n",
    "# transmat = np.array([[0.37, 0.63],\n",
    "#                     [0.63, 0.37]])\n",
    "\n",
    "transmat = np.array([[0.15, 0.85],\n",
    "                    [0.85, 0.15]])\n",
    "\n",
    "start_probs = np.array([0.5,0.5])\n",
    "\n",
    "emission_probabilities = np.array([[0.10,0.10,0.40,0.40],[0.35,0.35,0.15,0.15]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=2)\n",
    "model.startprob_ = start_probs \n",
    "model.transmat_ = transmat \n",
    "model.emissionprob_ = emission_probabilities\n",
    "\n",
    "X, Z = model.sample(1000)\n",
    "\n",
    "state2color = {} \n",
    "state2color[0] = 'black'\n",
    "state2color[1] = 'white'\n",
    "plot_DNA_samples(Z, state2color, 'states')\n",
    "\n",
    "samples = [item for sublist in X for item in sublist]\n",
    "obj2color = {} \n",
    "obj2color[0] = 'red'\n",
    "obj2color[1] = 'green'\n",
    "obj2color[2] = 'yellow'\n",
    "obj2color[3] = 'blue'\n",
    "plot_DNA_samples(samples, obj2color, 'observations')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6 (Expected) -1 point\n",
    "\n",
    "Generate 10000 samples using the defined hmm for generating DNA sequences. Learn the HMM in an unsupervised fashion similarly to what we did with the weather example i.e only use the observation samples not the \"hidden\" states. Constrast the original HMM to the HMM estimated from the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix\n",
      "Estimated model\n",
      "[[0.12851978 0.87148022]\n",
      " [0.83944903 0.16055097]]\n",
      "Original Model\n",
      "[[0.15 0.85]\n",
      " [0.85 0.15]]\n",
      "Emission probabilities\n",
      "Estimated model\n",
      "[[0.34361305 0.35384603 0.13982696 0.16271396]\n",
      " [0.1095737  0.09932463 0.41737085 0.37373081]]\n",
      "Original model\n",
      "[[0.1  0.1  0.4  0.4 ]\n",
      " [0.35 0.35 0.15 0.15]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "m1, m2 = model.sample(10000)\n",
    "est_model = hmm.MultinomialHMM(n_components = 2, n_iter = 1000000).fit(m1)\n",
    "\n",
    "m3 = est_model.predict(m1)\n",
    "state2color = {} \n",
    "state2color[1] = 'black'\n",
    "state2color[0] = 'white'\n",
    "\n",
    "\n",
    "print('Transition matrix')\n",
    "print(\"Estimated model\")\n",
    "print(est_model.transmat_)\n",
    "print('Original Model')\n",
    "print(model.transmat_)\n",
    "print(\"Emission probabilities\")\n",
    "print(\"Estimated model\")\n",
    "print(est_model.emissionprob_)\n",
    "print(\"Original model\")\n",
    "print(model.emissionprob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7 (Expected) -1 point\n",
    "\n",
    "Write a function called **classification_accuracy** that takes as input \n",
    "two arrays or lists of states and returns the number of states that are the same in both lists as a percentage. \n",
    "\n",
    "Consider the original sequences of states of the generated samples \n",
    "as ground truth. Then use the estimated model from the previous \n",
    "question to generate predicted states from the observation samples. \n",
    "That is the maximum likelihood sequence estimation problem. \n",
    "Note that the predicted states might be inverted compared to the original and you need to deal with that in your code (see the class notebook for details). Now compute the accuracy between the predicted \n",
    "sequence of states and the ground truth sequence of states. \n",
    "This is similar to the visual comparison of the original and predicted states in the provided notebook but using a quantified \n",
    "metric rather than a visualization. \n",
    "\n",
    "Now replace the transition model of the original HMM with a transition model that is all 0.5 i.e there is no transition information. Effectively this disregards any temporal dependenices and each time step is decided independently. In fact it corresponds to a Naive Bayes classifier with a single feature which is the nucleobase observation. \n",
    "\n",
    "What is the classification accuracy in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: percent accuracy is: 50.00\n",
      "2: percent accuracy is: 0.00\n",
      "3: percent accuracy is: 0.00\n",
      "4: percent accuracy is: 50.00\n",
      "5: percent accuracy is: 0.00\n",
      "6: percent accuracy is: 0.00\n",
      "7: percent accuracy is: 0.00\n",
      "8: percent accuracy is: 0.00\n",
      "9: percent accuracy is: 50.00\n",
      "10: percent accuracy is: 0.00\n",
      "11: percent accuracy is: 0.00\n",
      "12: percent accuracy is: 0.00\n",
      "13: percent accuracy is: 50.00\n",
      "14: percent accuracy is: 50.00\n",
      "15: percent accuracy is: 50.00\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "#using a new batch of samples from the model for each iterration of testing \n",
    "\n",
    "#test1 = model.sample(10)\n",
    "\n",
    "\n",
    "#print(test1[1][0])\n",
    "\n",
    "def classification_accuracy(arr1, arr2):\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    for i in range(len(arr1)):\n",
    "        if arr1[1][i] == arr2[1][i]:\n",
    "            correct += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            total += 1\n",
    "    acc = correct / total\n",
    "    if acc < 0.5:\n",
    "        return acc\n",
    "    else:\n",
    "        inv_corr = 0\n",
    "        inv_tot = 0\n",
    "        inv_indx = len(arr2)-1\n",
    "        for i in range(len(arr1)):\n",
    "            if arr1[1][i] == arr2[1][inv_indx]:\n",
    "                inv_corr += 1\n",
    "                inv_tot += 1\n",
    "                inv_indx -= 1\n",
    "            else:\n",
    "                inv_tot += 1\n",
    "                inv_indx -= 1\n",
    "        acc = inv_corr / inv_tot\n",
    "        return acc\n",
    "    \n",
    "\n",
    "for i in range(15):  \n",
    "    X = est_model.sample(10000)\n",
    "    Y = model.sample(10000)\n",
    "    acc = classification_accuracy(Y, X)\n",
    "    print('{}: percent accuracy is: {:.2f}'.format(i+1, acc * 100 ))\n",
    "\n",
    "test1 = model.sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8 (Advanced) -1 point\n",
    "\n",
    "This question is a bit more open ended, will require some creativity and extra work. Consider the following problem: during your day your cell phone collects location data in terms of x,y coordinates. You do different activities such as going to university, eating, going to the gym. These activities take place in particular locations such as Restaurant A and Restaurant B or Gym A, Gym B and each particular location can be thought of as a two-dimensional Gaussian distribution of location points. If you consider the activity as the hidden state and the location as the observation you have a Hidden Markov Model. Because activities take place in multiple locations you can model this as a Gaussian Mixture Model (GMM). Each Gaussian will be multivariate 2D Gaussian distribution characterized by two means and and a 2 by 2 covariance matrix.\n",
    "\n",
    "Consider a hypothetical scenario with 3 activities (eat, study, exercise) and 3 locations (GMM components) for each activity. You will need to do some reading about how GMMs work. You can come up \n",
    "with reasonable estimates for the associated parameters. \n",
    "\n",
    "Basically the goal is the follow the format of the Markov Chain and HMM notebook and create appropriate visualizations using this problem.\n",
    "\n",
    "Visualize on a 2D plane using circles the different locations and corresponding mixture components\n",
    "Generate a dataset using a Hidden Markov Model of the problem\n",
    "Visualize the dataset on a 2D plane\n",
    "Show how you can learn the parameters of this HMM using https://hmmlearn.readthedocs.io/en/latest/api.html#hmmlearn.hmm.GMMHMM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9 (Basic) - 1 point\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this question is to get some familiarity with scikit-learn: https://scikit-learn.org/stable/\n",
    "\n",
    "Replicate movie review classification from the previous assignment using bernoulli Naive Bayes in sklearn. This is relatively straightforward you simply need to create appropriate binary feature matrix and labels. Report on the classification accuracy and confusion matrix for that problem using 3-fold cross-validation. \n",
    "You will need to consult the execllent sklearn documentation for details. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reporting accuracy for BernoulliNB with 3 K-fold for movie review classification is:\n",
      "[0.68065967 0.67466267 0.66666667]\n",
      "Classification wiht BernoulliNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.70      0.60      0.65      1000\n",
      "     class 1       0.65      0.75      0.70      1000\n",
      "\n",
      "    accuracy                           0.67      2000\n",
      "   macro avg       0.68      0.67      0.67      2000\n",
      "weighted avg       0.68      0.67      0.67      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "# this question assumes that the pos.zip and neg.zip files are in the same directory as the notebook. The file classification\n",
    "#uses them to create the binary array data used in the BernoulliNB classification. \n",
    "import os \n",
    "import zipfile\n",
    "import numpy as np \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "words = ['awful', 'bad', 'boring', 'dull', 'effective', 'great', 'hilarious']\n",
    "\n",
    "def env_setup(path, new_path):\n",
    "    with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(new_path)\n",
    "\n",
    "        \n",
    "        \n",
    "def vector_classification(target_words, folder, binV =None):\n",
    "    wc_dict = {k:0 for k in target_words}\n",
    "    bin_vectors = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            bin_vector = [0 for i in range(len(target_words))]\n",
    "            try:\n",
    "                with open(os.path.join(folder, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        for word in line.split():\n",
    "                            if word in target_words:\n",
    "                                bin_vector[target_words.index(word)] = 1\n",
    "                            else:\n",
    "                                pass\n",
    "                    bin_vectors.append(bin_vector)\n",
    "            except IsADirectoryError:\n",
    "                print('The given directory {} or {} is not in this directory.'.format(folder, filename))\n",
    "                continue\n",
    "            except FileNotFoundError:\n",
    "                print('The file is not in this directory.')\n",
    "                continue \n",
    "        else:\n",
    "            continue \n",
    "    for i in bin_vectors:\n",
    "        count = 0\n",
    "        for j in i:\n",
    "            if j == 1:\n",
    "                wc_dict[words[count]] += 1\n",
    "                count += 1\n",
    "                continue \n",
    "            else: \n",
    "                count += 1\n",
    "                continue \n",
    "    if binV == True:\n",
    "        probs = {}\n",
    "        for k,v in wc_dict.items():\n",
    "            probs[k] = round((v/len(bin_vectors)*100),2)\n",
    "        return probs\n",
    "    else:\n",
    "        return bin_vectors\n",
    "\n",
    "\n",
    "def array_def(arr1, arr2):\n",
    "    a = np.zeros(shape=(2000,7))\n",
    "    for i in range(len(arr1)):\n",
    "        a[i] = arr1[i]\n",
    "    secondary = 0\n",
    "    for j in range(2000 - len(arr2)):\n",
    "        a[1000+j] = arr2[secondary]\n",
    "        secondary+=1\n",
    "    return a\n",
    "\n",
    "\n",
    "path = './pos.zip'\n",
    "new_path = './pos'\n",
    "env_setup(path, new_path)\n",
    "path = './neg.zip'\n",
    "new_path = './neg'\n",
    "env_setup(path, new_path)\n",
    "\n",
    "\n",
    "X = vector_classification(words, './pos/pos/')\n",
    "Y = vector_classification(words, './neg/neg/')\n",
    "\n",
    "\n",
    "pos_probs = vector_classification(words, './pos/pos/', binV=True) #dictionary of positive review word probabilities\n",
    "neg_probs = vector_classification(words, './neg/neg/', binV=True) #dictionary of negtive review word probabilities\n",
    "\n",
    "merged = array_def(X, Y) #create pos and neg array for bernoulliNB classification\n",
    "\n",
    "Y_1 = []\n",
    "for i in range(1000):\n",
    "    Y_1.append(1)\n",
    "for i in range(1000):\n",
    "    Y_1.append(0)\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "clf = BernoulliNB()\n",
    "print('Reporting accuracy for BernoulliNB with 3 K-fold for movie review classification is:')\n",
    "print(cross_val_score(clf, merged, Y_1, cv=k_fold, n_jobs=1))\n",
    "print('Classification wiht BernoulliNB:')\n",
    "clf.fit(merged,Y_1)\n",
    "pred = clf.predict(merged)\n",
    "target_names = ['Positive', 'Negative']\n",
    "print(classification_report(Y_1, pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10 (Expected) - 1 point \n",
    "\n",
    "The goal of this question is to give you some familiarity with having continuous features and comparing different classifiers. For this question use the breast cancer dataset from sklearn: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\n",
    "Train and compare three classifiers using this dataset using 3-fold \n",
    "cross-validation to calculate the classification accuracy and classification report: \n",
    "\n",
    "1. The Gaussian Naive Bayes classifier (with default parameters) \n",
    "(from sklearn.naive_bayes import GaussianNB) \n",
    "2. Linear support vector machine (with default parameters) \n",
    "(from sklearn.svm import LinearSVC) \n",
    "3. Decision tree (with default parameters) \n",
    "(from sklearn.tree import DecisionTreeClassifier)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification wiht GuassianNB:\n",
      "[0.92105263 0.94736842 0.94179894]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.95      0.89      0.92       212\n",
      "      benign       0.94      0.97      0.95       357\n",
      "\n",
      "    accuracy                           0.94       569\n",
      "   macro avg       0.94      0.93      0.94       569\n",
      "weighted avg       0.94      0.94      0.94       569\n",
      "\n",
      "Classification accuracy with LinearSVC:\n",
      "[0.95789474 0.62631579 0.92592593]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.89      0.91       212\n",
      "      benign       0.94      0.96      0.95       357\n",
      "\n",
      "    accuracy                           0.93       569\n",
      "   macro avg       0.93      0.93      0.93       569\n",
      "weighted avg       0.93      0.93      0.93       569\n",
      "\n",
      "Classification accuracy with DecisionTreeClassifier:\n",
      "[0.92105263 0.93684211 0.92063492]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       1.00      1.00      1.00       212\n",
      "      benign       1.00      1.00      1.00       357\n",
      "\n",
      "    accuracy                           1.00       569\n",
      "   macro avg       1.00      1.00      1.00       569\n",
      "weighted avg       1.00      1.00      1.00       569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "data = load_breast_cancer(return_X_y=True, as_frame=False)\n",
    "\n",
    "X = data[0] #data \n",
    "Y = data[1] #target\n",
    "\n",
    "\n",
    "\n",
    "print('Classification wiht GuassianNB:')\n",
    "k_fold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "clf = GaussianNB()\n",
    "print(cross_val_score(clf, X, Y, cv=k_fold, n_jobs=1))\n",
    "clf.fit(X,Y)\n",
    "pred = clf.predict(X)\n",
    "target_names = ['malignant', 'benign']\n",
    "print(classification_report(Y, pred, target_names=target_names))\n",
    "\n",
    "\n",
    "print('Classification accuracy with LinearSVC:')\n",
    "k_fold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "clf = LinearSVC()\n",
    "print(cross_val_score(clf, X, Y, cv=k_fold, n_jobs=1))\n",
    "clf.fit(X,Y)\n",
    "pred = clf.predict(X)\n",
    "target_names = ['malignant', 'benign']\n",
    "print(classification_report(Y, pred, target_names=target_names))\n",
    "\n",
    "print('Classification accuracy with DecisionTreeClassifier:')\n",
    "k_fold = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "clf = DecisionTreeClassifier()\n",
    "print(cross_val_score(clf, X, Y, cv=k_fold, n_jobs=1))\n",
    "clf.fit(X,Y)\n",
    "pred = clf.predict(X)\n",
    "target_names = ['malignant', 'benign']\n",
    "print(classification_report(Y,pred, target_names=target_names))\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
